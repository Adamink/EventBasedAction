import torch
import torch.nn as nn
class DHP_CNN(nn.Module):
    def __init__(self, input_size = (1, 260, 344)):
        # input: (batch, 1/3, 260, 344)
        # output: (batch, 13, 260, 344)
        super().__init__()
        c, h, w = input_size
        assert w==344, 'wrong input size!'
        self.conv1 = nn.Conv2d(c, 16, 3, padding = 1, bias = False)
        self.activation_1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(2)
        self.conv2a = nn.Conv2d(16, 32, 3, padding = 1, bias = False)
        self.activation_2 = nn.ReLU()
        self.conv2b = nn.Conv2d(32, 32, 3, padding = 1, bias = False)
        self.activation_3 = nn.ReLU()
        self.conv2d = nn.Conv2d(32, 32, 3, padding = 1, bias = False)
        self.activation_4 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(2)
        # (65, 86)
        self.conv3a = nn.Conv2d(32, 64, 3, padding = 2, dilation = 2, bias = False)
        self.activation_5 = nn.ReLU()
        self.conv3b = nn.Conv2d(64, 64, 3, padding = 2, dilation = 2, bias = False)
        self.activation_6 = nn.ReLU()
        self.conv3c = nn.Conv2d(64, 64, 3, padding = 2, dilation = 2, bias = False)
        self.activation_7 = nn.ReLU()
        self.conv3d = nn.Conv2d(64, 64, 3, padding = 2, dilation = 2, bias = False)
        self.activation_8 = nn.ReLU()
        if w==346:
            self.conv3_up = nn.ConvTranspose2d(64, 32, 3, stride = 2, padding = (1,0), output_padding = (1,0), bias = False)
        else:
            self.conv3_up = nn.ConvTranspose2d(64, 32, 3, stride = 2, padding = 1, output_padding = 1, bias = False)
        self.activation_9 = nn.ReLU()
        self.conv4a = nn.Conv2d(32, 32, 3, padding = 2, dilation = 2, bias = False)
        self.activation_10 = nn.ReLU()
        self.conv4b = nn.Conv2d(32, 32, 3, padding = 2, dilation = 2, bias = False)
        self.activation_11 = nn.ReLU()
        self.conv4c = nn.Conv2d(32, 32, 3, padding = 2, dilation = 2, bias = False)
        self.activation_12 = nn.ReLU()
        self.conv4d = nn.Conv2d(32, 32, 3, padding = 2, dilation = 2, bias = False)
        self.activation_13 = nn.ReLU()
        self.conv4_up = nn.ConvTranspose2d(32, 16, 3, stride = 2, padding = 1, output_padding = 1, bias = False)
        self.activation_14 = nn.ReLU()
        self.conv5a = nn.Conv2d(16, 16, 3, padding = 1, bias = False)
        self.activation_15 = nn.ReLU()
        self.conv5d = nn.Conv2d(16, 16, 3, padding = 1, bias = False)
        self.activation_16 = nn.ReLU()
        self.pred_cube = nn.Conv2d(16, 13, 3, padding = 1, bias = False)
        self.activation_17 = nn.ReLU()
    def forward(self,x):
        x = self.conv1(x)
        x = self.activation_1(x)
        x = self.pool1(x)
        x = self.conv2a(x)
        x = self.activation_2(x)
        x = self.conv2b(x)
        x = self.activation_3(x)
        x = self.conv2d(x)
        x = self.activation_4(x)
        x = self.pool2(x)
        # print(x.size())
        x = self.conv3a(x)
        x = self.activation_5(x)
        x = self.conv3b(x)
        x = self.activation_6(x)
        x = self.conv3c(x)
        x = self.activation_7(x)
        x = self.conv3d(x)
        x = self.activation_8(x)
        x = self.conv3_up(x)
        x = self.activation_9(x)
        x = self.conv4a(x)
        x = self.activation_10(x)
        x = self.conv4b(x)
        x = self.activation_11(x)
        x = self.conv4c(x)
        x = self.activation_12(x)
        x = self.conv4d(x)
        x = self.activation_13(x)
        x = self.conv4_up(x)
        x = self.activation_14(x)
        x = self.conv5a(x)
        x = self.activation_15(x)
        x = self.conv5d(x)
        x = self.activation_16(x)
        x = self.pred_cube(x)
        x = self.activation_17(x)
        return x

if __name__ == '__main__':
    input_size = (1, 260, 344)
    a = torch.zeros((16,) + input_size)
    m = DHP_CNN(input_size)
    b = m(a)
    print(b.size())